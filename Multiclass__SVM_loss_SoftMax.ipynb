{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiclass _SVM_loss_SoftMax.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPS8ux7Y4UUxbPd6ypWsxIf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crazyboyonline/data-structure/blob/main/Multiclass__SVM_loss_SoftMax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code. Here is the loss function (without regularization) implemented in Python, in both unvectorized and half-vectorized form:"
      ],
      "metadata": {
        "id": "0UVELYf_d80m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeXE-aHLKCyf"
      },
      "outputs": [],
      "source": [
        "def L_i(x,y,W):\n",
        "    \"\"\"\n",
        "    unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
        "    - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
        "      with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
        "    - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
        "    - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
        "    \"\"\"\n",
        "    delta = 1.0#see notes about delta later in this section\n",
        "    scores = W.dot(x)#scores becomes of size 10 x 1, the scores for each class\n",
        "    correct_class_score = scores[y]\n",
        "    D = W.shape[0]#number of classes \n",
        "    loss_i = 0,0\n",
        "    for j in range(D):#iterate over all wrong classes\n",
        "      if j == y:\n",
        "        #skip for the true class to only loop over incorrect classes\n",
        "        continue\n",
        "        #accumulate loss for the i- th example\n",
        "      loss_i += max(0,scores[j] - correct_class_score + delta)\n",
        "    return loss_i\n",
        "\n",
        "\n",
        "def L_i_vectorized(x,y,W):\n",
        "  \"\"\"\n",
        "  A faster half-vectorized implementation. half-vectorized\n",
        "  refers to the fact that for a single example the implementation contains\n",
        "  no for loops, but there is still one loop over the examples (outside this function)\n",
        "  \"\"\"\n",
        "  delta = 1.0\n",
        "  scores = W.dot(x)\n",
        "  # compute the margins for all classes in one vector operation\n",
        "  margins = np.maximum(0,scores - scores[y] + delta)\n",
        "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
        "  # to ignore the y-th position and only consider margin on max wrong class\n",
        "  margins[y] = 0\n",
        "  loss_i = np.sum(margins)\n",
        "  return loss_i\n",
        "\n",
        "def L(X,y,W):\n",
        "  \"\"\"\n",
        "  fully-vectorized implementation :\n",
        "  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
        "  - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
        "  - W are weights (e.g. 10 x 3073)\n",
        "  \"\"\"\n",
        "  # evaluate loss over all examples in X without using any for loops\n",
        "  # left as exercise to reader in the assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Softmax classifier"
      ],
      "metadata": {
        "id": "e8OOW0E6ie4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "f = np.array([123,456,789])#example with 3 classes and each having large scores\n",
        "p = np.exp(f) / np.sum(np.exp(f))\n",
        "# instead: first shift the values of f so that the highest number is 0:\n",
        "f -= np.max(f) # f becomes [-666, -333, 0]\n",
        "p = np.exp(f) / np.sum(np.exp(f))# safe to do, gives the correct answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km95JrWVijnC",
        "outputId": "77dc400d-c210-43f6-8b73-7da0f573e3ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    }
  ]
}